{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9fd8bd-b884-4b59-bac0-8e3256306b5f",
   "metadata": {},
   "source": [
    "<center><h1>WEB-SCRAPING_2_Selenium-Practice_Questions</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2cf146c-6af7-4ae8-a8f9-5bb4306b4eaf",
   "metadata": {},
   "source": [
    "! pip install --upgrade selenium\n",
    "! pip install --upgrade time\n",
    "! pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a180002-bfb8-45fa-867c-31adda3c23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10aa3752-81e1-4815-8672-76bae912d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_browser(url):\n",
    "    try:\n",
    "        # Set up the webdriver\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening the browser: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff93b6c-9886-4634-b4dc-5d1618bb57f4",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd02d2c-533f-47b7-a1a7-b57ef088306c",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "You have to scrape the job-title, job-location, company_name, experience_required. You have to \n",
    "scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5fd1296-1b0a-407a-afd1-7fffde155517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title       Location  \\\n",
      "0  Data Analyst , Senior Data Analyst , Data Anal...  Bangalore\\n+8   \n",
      "1                                       Data Analyst      Bangalore   \n",
      "2                                       Data Analyst  Bangalore\\n+4   \n",
      "3    Risk Data Analyst (Senior Quantitative Analyst)  Bangalore\\n+1   \n",
      "4                            Healthcare Data Analyst  Bangalore\\n+8   \n",
      "5                     Urgent Hiring For Data Analyst  Bangalore\\n+2   \n",
      "6                                     Data Scientist      Bangalore   \n",
      "7            Data Engineer (Lead) - Wipro) PAN India  Bangalore\\n+6   \n",
      "8                   Data Catalog with Data Goverance  Bangalore\\n+8   \n",
      "9                      Business Intelligence Analyst  Bangalore\\n+8   \n",
      "\n",
      "                                  Company    Experience  \n",
      "0                       appsoft solutions    0 to 4 Yrs  \n",
      "1  phoenix global re settlement servic...    3 to 8 Yrs  \n",
      "2                        aryan technology    0 to 4 Yrs  \n",
      "3                       locus enterprises    4 to 9 Yrs  \n",
      "4               spento papers (india) llp   8 to 13 Yrs  \n",
      "5  diraa hr services hiring for diraa ...    0 to 4 Yrs  \n",
      "6  phoenix global re settlement servic...   6 to 11 Yrs  \n",
      "7           wroots global private limited    6 to 9 Yrs  \n",
      "8                     ltimindtree limited   6 to 11 Yrs  \n",
      "9               spento papers (india) llp  14 to 23 Yrs  \n"
     ]
    }
   ],
   "source": [
    "# opening Shine website\n",
    "driver = open_browser('https://www.shine.com/')\n",
    "time.sleep(5)\n",
    "\n",
    "# Navigating to the search results page\n",
    "try:\n",
    "    # opening input prompt.\n",
    "    driver.find_element(By.CLASS_NAME,\"input\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Entering \"Data Analyst\"\n",
    "    driver.find_element(By.XPATH, \"//input[@id='id_q']\").send_keys('Data Analyst')\n",
    "    time.sleep(2) # Waiting\n",
    "\n",
    "    # Entering \"Bangalore\"\n",
    "    driver.find_element(By.ID, 'id_loc').send_keys('Bangalore')\n",
    "    time.sleep(2) # Waiting\n",
    "\n",
    "    # Clicking the search button\n",
    "    driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "    time.sleep(2) # Waiting\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while navigating: \", e)\n",
    "\n",
    " # Creating a empty list to hold the data\n",
    "jobs = []\n",
    "    \n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Scrape job data for the first 10 results\n",
    "        job_titles = driver.find_elements(By.XPATH, \"//strong[@class='jobCard_pReplaceH2__xWmHg']\")\n",
    "        locations = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2']\")\n",
    "        companies = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_cName__mYnow']\")\n",
    "        experiences = driver.find_elements(By.XPATH, \"//div[@class=' jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t']\")\n",
    "        \n",
    "        # Append the scraped data to the jobs list\n",
    "        job = {\n",
    "            'Job Title': job_titles[i].text,\n",
    "            'Location': locations[i].text,\n",
    "            'Company': companies[i].text,\n",
    "            'Experience': experiences[i].text\n",
    "        }\n",
    "        jobs.append(job)\n",
    "\n",
    "except Exception:\n",
    "    print(\"Error occurred while scraping job data.\")\n",
    "\n",
    "\n",
    "# Create a DataFrame and print the data\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Printing the results\n",
    "print(df)\n",
    "\n",
    "# Saving the results to a CSV file\n",
    "df.to_csv('shine_job.csv', index=False)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4145cb-380e-4ce3-ba58-4676df153dcd",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348cc44-2def-4597-ad86-f4f3b38f6c01",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ““Data Scientist” Job position in “Bangalore” location. \n",
    "You have to scrape the job-title, job-location, company_name, experience_required. You have to \n",
    "scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data. <br>\n",
    "**Note: All of the above steps have to be done in code. No step is to be done manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73e47414-4abc-4d50-9c7d-2c1ff7d99835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Job Title  \\\n",
      "0          Data Analyst (Tamil/ English)   \n",
      "1                           Data Analyst   \n",
      "2                           Data Analyst   \n",
      "3                           Data Analyst   \n",
      "4                     GPSG- Data Analyst   \n",
      "5                           Data Analyst   \n",
      "6           Data Analyst - Python/Hadoop   \n",
      "7                       Data Analyst III   \n",
      "8  Data Analyst-Direct/Indirect Taxation   \n",
      "9                           Data Analyst   \n",
      "\n",
      "                                            Location            Company  \\\n",
      "0  Bengaluru, Kolkata, Mumbai, New Delhi, Hyderab...     Zbiz Solutions   \n",
      "1                                          Bengaluru                ANZ   \n",
      "2                                          Bengaluru      Bluestone.com   \n",
      "3  Bengaluru, Kolkata, Mumbai, New Delhi, Hyderab...  Faptic Technology   \n",
      "4                                          Bengaluru                IBM   \n",
      "5                                          Bengaluru           Homelane   \n",
      "6                                          Bengaluru         Sadup Soft   \n",
      "7                                          Bengaluru         Sadup Soft   \n",
      "8                                          Bengaluru             Oracle   \n",
      "9                                          Bengaluru              Optum   \n",
      "\n",
      "  Experience  \n",
      "0    0-3 Yrs  \n",
      "1    1-5 Yrs  \n",
      "2    2-4 Yrs  \n",
      "3    1-4 Yrs  \n",
      "4    1-4 Yrs  \n",
      "5    2-5 Yrs  \n",
      "6    3-6 Yrs  \n",
      "7    5-8 Yrs  \n",
      "8    1-6 Yrs  \n",
      "9    2-5 Yrs  \n"
     ]
    }
   ],
   "source": [
    "# Opening the Naukri website\n",
    "driver = open_browser('https://www.naukri.com/')\n",
    "\n",
    "try:\n",
    "    # searching for 'Data Analyst'\n",
    "    designation = driver.find_element(By.CLASS_NAME, 'suggestor-input')\n",
    "    designation.send_keys('Data Analyst')\n",
    "\n",
    "    # clicking 'search button'\n",
    "    search = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "    search.click()\n",
    "\n",
    "    time.sleep(5) # Waiting\n",
    "\n",
    "    # Applying Location filters\n",
    "    location_catagory = driver.find_element(By.XPATH, \"/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[4]/div[2]/div[1]/label/p/span[1]\")\n",
    "    location_catagory.click()\n",
    "\n",
    "    time.sleep(5)  # Waiting\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while navigating:\", e )\n",
    "\n",
    "# List to store job details\n",
    "jobs = []\n",
    "\n",
    "# Extracting the job details\n",
    "try:\n",
    "    # Collect data for the first 10 jobs\n",
    "    for i in range(10):\n",
    "        # Extracting job details and storeing it in a list of dictionaries\n",
    "        title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title \"]')\n",
    "        location_tags = driver.find_elements(By.XPATH, '//span[@class=\"locWdth\"]')\n",
    "        company_tags = driver.find_elements(By.XPATH, '//div[@class=\" row2\"]/span/a[1]')\n",
    "        experience_tags = driver.find_elements(By.XPATH, '//span[@class=\"expwdth\"]')\n",
    "\n",
    "        jobs.append({\n",
    "            'Job Title': title_tags[i].text.strip(),\n",
    "            'Location': location_tags[i].text.strip(),\n",
    "            'Company': company_tags[i].text.strip(),\n",
    "            'Experience': experience_tags[i].text.strip() \n",
    "        })\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while extracting data:\", e)\n",
    "\n",
    "# Createing a DataFrame\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('naukri_jobs.csv', index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a498a77-e4e0-40f9-9076-b1ed1c820d9a",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0cf6c-cd47-45e9-93cf-244d47b88f8f",
   "metadata": {},
   "source": [
    "Q3: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand \n",
    "2. ProductDescription \n",
    "3. Price\n",
    "\n",
    "To scrape the data you have to go through following steps: \n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and \n",
    "click the search icon \n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the \n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then \n",
    "click on it. \n",
    "5. Now scrape data from this page as usual \n",
    "6. Repeat this until you get data for 100sunglasses. Note: That all of the above steps have to be done\n",
    "by coding only and not manually.\n",
    "**Note: That all of the above steps have to be done by coding only and not manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e024948-4be6-43dc-bc93-0b20d6a4128e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Brand                            Product Description    Price\n",
      "0   Fastrack   UV Protection Aviator Sunglasses (Free Size)     ₹399\n",
      "1   Fastrack  UV Protection Wayfarer Sunglasses (Free Size)     ₹399\n",
      "2    Ray-Ban             Polarized Wayfarer Sunglasses (55)  ₹11,059\n",
      "3   OPTRICKS             Others Rectangular Sunglasses (16)     ₹330\n",
      "4  ROYAL SON               Mirrored Aviator Sunglasses (58)     ₹375\n",
      "             Brand                                Product Description Price\n",
      "95          PIRASO              UV Protection Aviator Sunglasses (58)  ₹248\n",
      "96   VINCENT CHASE  by Lenskart Polarized, UV Protection Clubmaste...  ₹909\n",
      "97       ROYAL SON  UV Protection Rectangular, Retro Square Sungla...  ₹449\n",
      "98  ROZZETTA CRAFT              UV Protection Aviator Sunglasses (62)  ₹429\n",
      "99      Dressberry         UV Protection Round Sunglasses (Free Size)  ₹339\n"
     ]
    }
   ],
   "source": [
    "# Opening the Flipkart website\n",
    "driver = open_browser('https://www.flipkart.com')\n",
    "\n",
    "# Waiting\n",
    "time.sleep(2)\n",
    "\n",
    "# Search for sunglasses\n",
    "search_input = driver.find_element(By.NAME, 'q')\n",
    "search_input.send_keys('sunglasses')\n",
    "search_input.send_keys('\\n')\n",
    "\n",
    "time.sleep(3)  # Waiting\n",
    "\n",
    "# Creating empty lists\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Initial page number\n",
    "i = 1\n",
    "\n",
    "# Loop to scrape data until 100 sneakers are gathered\n",
    "while len(brands) < 100:\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find sneaker blocks\n",
    "    sneaker_blocks = soup.find_all('div', class_='hCKiGj')\n",
    "\n",
    "    for block in sneaker_blocks:\n",
    "        if len(brands) >= 100:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Extract the brand, description, and price using BeautifulSoup\n",
    "            brand = block.find('div', class_='syl9yP').text.strip()  # Finding the brand\n",
    "            description = block.find('a', class_='WKTcLC BwBZTg').text.strip()  # Finding the product description\n",
    "            price = block.find('div', class_='Nx9bqj').text.strip()  # Finding the price\n",
    "\n",
    "            # Append data to lists\n",
    "            brands.append(brand)\n",
    "            descriptions.append(description)\n",
    "            prices.append(price)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Navigate to the next page if needed\n",
    "    try:\n",
    "        i += 1\n",
    "        next_button = driver.find_element(By.XPATH, f\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[{i}]\")  # Assuming this points to the next page\n",
    "        if next_button:\n",
    "            next_button.click()\n",
    "            time.sleep(3)  # Waiting\n",
    "    except Exception as e:\n",
    "        print(f\"Next page navigation error: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Brand': brands,\n",
    "    'Product Description': descriptions,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('flipkart_sunglasses.csv', index=False)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5067f7a-9fcf-4d37-bbf5-a2ee0895fd24",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
