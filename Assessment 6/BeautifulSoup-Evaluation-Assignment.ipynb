{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98cf7820-25be-4006-9a8c-51592f4c97c0",
   "metadata": {},
   "source": [
    "<center><h1>BeautifulSoup-Evaluation-Assignment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd99d5fd-9421-4c90-a2dd-bad66367ccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: requests in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Installing and Updating the requierd library.\n",
    "! pip install --upgrade pandas \n",
    "! pip install --upgrade bs4 \n",
    "! pip install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e21fb5-758b-4b74-bd28-dc72486db82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necacery library and setting some pre requsits.\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8849aebb-a02e-4e94-9a9a-f957d9bf375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fuction to fetch the responce of the Website\n",
    "def fetch(url):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "        \"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"\n",
    "    }\n",
    "    response = pls.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"HTTP error! Status: {response.status_code}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5527222-cf4c-40a9-bc1e-48bdbb1fbeba",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "id": "529c76ba-239d-4d9b-a532-1b2c7ea4025b",
   "metadata": {},
   "source": [
    "1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfbdc49-3a82-4af8-a391-f0c9a676cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Name Rating  Year\n",
      "0                    1. Ship of Theseus    8.0  2012\n",
      "1                             2. Iruvar    8.4  1997\n",
      "2                    3. Kaagaz Ke Phool    7.8  1959\n",
      "3  4. Lagaan: Once Upon a Time in India    8.1  2001\n",
      "4                    5. Pather Panchali    8.2  1955\n",
      "5                          6. Charulata    8.1  1964\n",
      "6                    7. Rang De Basanti    8.1  2006\n",
      "7                              8. Dev.D    7.9  2009\n",
      "8                           9. 3 Idiots    8.4  2009\n",
      "9                            10. Awaara    7.8  1951\n"
     ]
    }
   ],
   "source": [
    "# Setting URL to IMDB List URL\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "# Fetching the response from URL\n",
    "response = fetch(url)\n",
    "\n",
    "# Parsing the HTML response\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Empty lists to store movie names, ratings, and years\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "# Iterating over each item in the movie details\n",
    "for item in soup.find_all('li', class_=\"ipc-metadata-list-summary-item\"):\n",
    "    \n",
    "    # Extracting the movie name\n",
    "    name = item.find('h3', class_=\"ipc-title__text\").text.strip()  \n",
    "    \n",
    "    # Extracting the movie rating\n",
    "    rating = item.find('span', class_=\"ipc-rating-star--rating\").text.strip()  \n",
    "    \n",
    "    # Extracting the movie year\n",
    "    year = item.find('span', class_=\"sc-ab348ad5-8 cSWcJI dli-title-metadata-item\").text.strip().strip('()')  \n",
    "\n",
    "    # Appending the extracted data\n",
    "    names.append(name)\n",
    "    ratings.append(rating)\n",
    "    years.append(year)\n",
    "\n",
    "# Creating a DataFrame \n",
    "movies_df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Rating': ratings,\n",
    "    'Year': years\n",
    "})\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "movies_df.to_csv('movies.csv', index=False)  \n",
    "\n",
    "# Printing the DataFrame\n",
    "print(movies_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5f57b-09fd-433b-b814-ec951f6e6cd8",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dc2edce-d3db-4df7-ba7b-261a4c1fe508",
   "metadata": {},
   "source": [
    "2) Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91698cc3-74f4-4cd2-804f-1268fe37901e",
   "metadata": {},
   "source": [
    "Sorry, I tried solving this for more than two nights and still couldn’t get the correct output. However, it can be easily solved with Selenium but then also this is questionable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca786f2-1cbb-46c1-8c66-5cdbb245b614",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36160e8f-b99d-4a59-b473-7ca5446f88e0",
   "metadata": {},
   "source": [
    "3) Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a2bd4-ed7d-4be5-8465-7c23ba4e640a",
   "metadata": {},
   "source": [
    "Sorry, I tried solving this for more than two nights and still couldn’t get the correct output. However, it can be easily solved with Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a7bbd-4863-4771-b0ea-f04be3ea9d79",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df286821-438d-41eb-9367-5e496e1d03b6",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "https://www.bewakoof.com/bestseller?sort=popular ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279648cb-3796-40fc-95c2-903f80caf076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Product Name  Price  \\\n",
      "0  Bewakoof®Women's Black Whatever Cat Graphic Pr...  ₹1099   \n",
      "1  Bewakoof®Men's Purple Seek Balance Graphic Pri...  ₹1499   \n",
      "2  Bewakoof®Men's Blue Rider Vroom Panda Graphic ...  ₹1099   \n",
      "3  bewakoof x tom & jerryWomen's Green Sundays We...  ₹1099   \n",
      "4       Bewakoof®Men's Black Wander Geometry T-shirt  ₹1099   \n",
      "5  Bewakoof®Women's Red Inner Peace Graphic Print...   ₹999   \n",
      "6  Bewakoof®Men's Purple Beast Within Graphic Pri...  ₹1499   \n",
      "7  bewakoof x marvelMen's Black Iron Man Of War G...  ₹1099   \n",
      "8  bewakoof x dcMen's Black The Dark Knight Graph...  ₹1499   \n",
      "9  bewakoof x disneyWomen's Red Sarcastic One (DL...  ₹1099   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://images.bewakoof.com/t640/women-s-black...  \n",
      "1  https://images.bewakoof.com/t640/men-s-purple-...  \n",
      "2  https://images.bewakoof.com/t640/men-s-blue-ri...  \n",
      "3  https://images.bewakoof.com/t640/women-s-green...  \n",
      "4  https://images.bewakoof.com/t640/men-s-black-w...  \n",
      "5  https://images.bewakoof.com/t640/women-s-red-i...  \n",
      "6  https://images.bewakoof.com/t640/men-s-purple-...  \n",
      "7  https://images.bewakoof.com/t640/men-s-black-i...  \n",
      "8  https://images.bewakoof.com/t640/men-s-black-t...  \n",
      "9  https://images.bewakoof.com/t640/women-s-red-s...  \n"
     ]
    }
   ],
   "source": [
    "# Define the URL to scrape\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Lists to hold product data\n",
    "product_name = []\n",
    "price = []\n",
    "images = []\n",
    "\n",
    "# Extract product names\n",
    "for i in soup.find_all('div', class_=\"productNaming bkf-ellipsis\"):\n",
    "    product_name.append(i.text.strip())\n",
    "\n",
    "# Extract product prices\n",
    "for i in soup.find_all('div', class_=\"actualPriceText clr-shade5\"):\n",
    "    price.append(i.text.strip())\n",
    "\n",
    "# Extract image URLs\n",
    "for i in soup.find_all(\"img\", class_=\"productImgTag\"):\n",
    "    images.append(i['src'])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': product_name,\n",
    "    'Price': price,\n",
    "    'Image URL': images\n",
    "})\n",
    "\n",
    "# Display the first 10 entries\n",
    "print(df.head(10))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('bewakoof_bestsellers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02d272-8427-4aaa-8f0a-d64874407d42",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "id": "855892dd-8c8c-49f4-a087-cc6bf639edbc",
   "metadata": {},
   "source": [
    "5) Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    " a) headings\n",
    " b) date\n",
    " c) News link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d2711ab-8802-4927-9e8c-1af074139f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Heading           Date  \\\n",
      "0   The Federal Reserve may have pretty much just ...  No date found   \n",
      "1   Dow jumps 400 points to a record on Friday, S&...  No date found   \n",
      "2   OCBC: Expect more fiscal stimulus from the Chi...  No date found   \n",
      "3   ‘She doesn't stop talking’: How extroverts unk...  No date found   \n",
      "4   Russia is on a mission to create mayhem on Eur...  No date found   \n",
      "5   'Golden Week’ bookings show Chinese travel pre...  No date found   \n",
      "6   Britain's Rightmove rejects $8.1 bln sweetened...  No date found   \n",
      "7   Fed rate cuts would 'undoubtedly' be positive ...  No date found   \n",
      "8   Watch CNBC's full interview with Malaysian Pri...  No date found   \n",
      "9   Johor-Singapore Special Economic Zone: 'Things...  No date found   \n",
      "10  'Full force': Malaysia PM says crusade against...  No date found   \n",
      "11  Fearful of the 'middle-income trap,' Indonesia...  No date found   \n",
      "12  He flies business and first class multiple tim...  No date found   \n",
      "13  ‘She doesn't stop talking’: How extroverts unk...  No date found   \n",
      "14  Chinese travelers trickle back to a Taiwanese ...  No date found   \n",
      "15  From guns to politics: See which factors deter...  No date found   \n",
      "16  Starlink to end the days of spotty Wi-Fi on pl...  No date found   \n",
      "17  This is the No. 1 way to raise kids with a 're...  No date found   \n",
      "18  You could miss out on years of 'free money' if...  No date found   \n",
      "19  Ex-Google exec: This simple habit separates hi...  No date found   \n",
      "20  3 money moves you can make today to feel wealt...  No date found   \n",
      "21  New research: More money tends to make you hap...  No date found   \n",
      "22                     What are the economics of war?  No date found   \n",
      "23                    What is the internet of bodies?  No date found   \n",
      "24       How the world got into $315 trillion of debt  No date found   \n",
      "25  eVTOLS: Are flying cars finally becoming reality?  No date found   \n",
      "26                  How China's property bubble burst  No date found   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.com/2024/10/11/the-federal-re...  \n",
      "1   https://www.cnbc.com/2024/10/10/stock-market-t...  \n",
      "2   https://www.cnbc.com/video/2024/10/09/ocbc-con...  \n",
      "3   https://www.cnbc.com/2024/10/09/she-wont-stop-...  \n",
      "4   https://www.cnbc.com/2024/10/08/russia-on-a-mi...  \n",
      "5   https://www.cnbc.com/2024/09/30/golden-week-bo...  \n",
      "6   https://www.cnbc.com/2024/09/25/rightmove-reje...  \n",
      "7   https://www.cnbc.com/video/2024/10/07/warburg-...  \n",
      "8   https://www.cnbc.com/video/2024/10/03/watch-cn...  \n",
      "9   https://www.cnbc.com/video/2024/09/30/johor-si...  \n",
      "10  https://www.cnbc.com/2024/09/30/anwar-ibrahim-...  \n",
      "11  https://www.cnbc.com/2024/09/26/indonesia-look...  \n",
      "12  https://www.cnbc.com/2024/10/10/he-flies-busin...  \n",
      "13  https://www.cnbc.com/2024/10/09/she-wont-stop-...  \n",
      "14  https://www.cnbc.com/2024/10/08/chinese-travel...  \n",
      "15  https://www.cnbc.com/2024/10/06/is-it-safe-to-...  \n",
      "16  https://www.cnbc.com/2024/10/04/starlink-to-en...  \n",
      "17  https://www.cnbc.com/2024/10/11/lighthouse-par...  \n",
      "18  https://www.cnbc.com/2024/10/11/you-could-miss...  \n",
      "19  https://www.cnbc.com/2024/10/11/former-google-...  \n",
      "20  https://www.cnbc.com/2024/10/11/money-moves-yo...  \n",
      "21  https://www.cnbc.com/2024/10/11/more-money-mak...  \n",
      "22  https://www.cnbc.com/video/2024/08/07/what-are...  \n",
      "23  https://www.cnbc.com/video/2024/05/31/what-is-...  \n",
      "24  https://www.cnbc.com/video/2024/05/28/how-the-...  \n",
      "25  https://www.cnbc.com/video/2024/03/28/evtols-h...  \n",
      "26  https://www.cnbc.com/video/2024/02/29/how-chin...  \n"
     ]
    }
   ],
   "source": [
    "# Define the URL to scrape\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = fetch(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all articles on the page\n",
    "articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "# Initialize lists to hold the article data\n",
    "headings = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "# Iterate through each article to extract the required information\n",
    "for article in articles:\n",
    "    # Extract the heading\n",
    "    heading_element = article.find('a', class_='Card-title')\n",
    "    heading = heading_element.text.strip() if heading_element else \"No heading found\"\n",
    "\n",
    "    # Extract the publication date\n",
    "    date_element = article.find('time', class_=\"LatestNews-timestamp\")\n",
    "    date = date_element.text.strip() if date_element else \"No date found\"\n",
    "\n",
    "    # Extract the news link\n",
    "    link_element = article.find('a', href=True)\n",
    "    news_link = link_element['href'] if link_element else \"No link found\"\n",
    "\n",
    "    # Append the extracted data to the respective lists\n",
    "    headings.append(heading)\n",
    "    dates.append(date)\n",
    "    links.append(news_link)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = {\n",
    "    'Heading': headings,\n",
    "    'Date': dates,\n",
    "    'Link': links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print( df )\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('cnbc_world_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e659e-d2e7-4da0-b228-b38214e234a1",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "id": "590cc3d1-a13a-4548-b785-7a4e4152cd66",
   "metadata": {},
   "source": [
    "6) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/ and scrap\n",
    "   a) Paper title\n",
    "   b) date\n",
    "   c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5b9bfb-fc85-4b4a-bbf2-8b525d56d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title            Date  \\\n",
      "0   Implementation of artificial intelligence in a...            2020   \n",
      "1   Automation and digitization of agriculture usi...            2021   \n",
      "2               Review of agricultural IoT technology            2022   \n",
      "3   Computer vision in smart agriculture and preci...  September 2024   \n",
      "4   A comprehensive review on automation in agricu...       June 2019   \n",
      "5   Comparing YOLOv8 and Mask R-CNN for instance s...  September 2024   \n",
      "6   Applications of electronic nose (e-nose) and e...            2020   \n",
      "7   A comprehensive survey on weed and crop classi...  September 2024   \n",
      "8   A review of imaging techniques for plant disea...            2020   \n",
      "9   Towards sustainable agriculture: Harnessing AI...       June 2024   \n",
      "10            Fruit ripeness classification: A survey      March 2023   \n",
      "11  Deep learning based computer vision approaches...            2022   \n",
      "12  Image classification on smart agriculture plat...  September 2024   \n",
      "13  Transfer Learning for Multi-Crop Leaf Disease ...            2022   \n",
      "14  DeepRice: A deep learning and deep feature bas...      March 2024   \n",
      "15  Plant disease detection using hybrid model bas...            2021   \n",
      "16  Using an improved lightweight YOLOv8 model for...      March 2024   \n",
      "17  Comparison of CNN-based deep learning architec...  September 2023   \n",
      "18  Cross-comparative review of Machine learning f...       June 2024   \n",
      "19  Deep convolutional neural network models for w...            2022   \n",
      "20  How artificial intelligence uses to achieve th...       June 2023   \n",
      "21  A systematic review of machine learning techni...            2022   \n",
      "22  Hyperparameter optimization of YOLOv8 for smok...       June 2024   \n",
      "23  Machine learning in nutrient management: A review  September 2023   \n",
      "24  Estimation of flea beetle damage in the field ...  September 2024   \n",
      "\n",
      "                                               Author  \n",
      "0   Tanha Talaviya |  Dhara Shah |  Nivedita Patel...  \n",
      "1                            A. Subeesh |  C.R. Mehta  \n",
      "2          Jinyuan Xu |  Baoxing Gu |  Guangzhao Tian  \n",
      "3   Sumaira Ghazal |  Arslan Munir |  Waqar S. Qur...  \n",
      "4   Kirtan Jha |  Aalap Doshi |  Poojan Patel |  M...  \n",
      "5      Ranjan Sapkota |  Dawood Ahmed |  Manoj Karkee  \n",
      "6                               Juzhong Tan |  Jie Xu  \n",
      "7   Faisal Dharma Adhinata |   Wahyono |  Raden Su...  \n",
      "8        Vijai Singh |  Namita Sharma |  Shikha Singh  \n",
      "9                 Dhananjay K. Pandey |  Richa Mishra  \n",
      "10  Matteo Rizzo |  Matteo Marcuzzo |  Alessandro ...  \n",
      "11  V.G. Dhanya |  A. Subeesh |  N.L. Kushwaha |  ...  \n",
      "12  Juan Felipe Restrepo-Arias |  John W. Branch-B...  \n",
      "13             Ananda S. Paymode |  Vandana B. Malode  \n",
      "14  P. Isaac Ritharson |  Kumudha Raimond |  X. An...  \n",
      "15                         Punam Bedi |  Pushkar Gole  \n",
      "16  Baoling Ma |  Zhixin Hua |  Yuchen Wen |  Hong...  \n",
      "17  Md Taimur Ahad |  Yan Li |  Bo Song |  Touhid ...  \n",
      "18  James Daniel Omaye |  Emeka Ogbuju |  Grace At...  \n",
      "19  A. Subeesh |  S. Bhole |  K. Singh |  N.S. Cha...  \n",
      "20            Vilani Sachithra |  L.D.C.S. Subhashini  \n",
      "21  Md Ekramul Hossain |  Muhammad Ashad Kabir |  ...  \n",
      "22  Leo Ramos |  Edmundo Casas |  Eduardo Bendek |...  \n",
      "23  Oumnia Ennaji |  Leonardus Vergütz |  Achraf E...  \n",
      "24  Arantza Bereciartua-Pérez |  María Monzón |  D...  \n"
     ]
    }
   ],
   "source": [
    "# Define the URL to scrape\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all articles on the page\n",
    "articles = soup.find_all('div', class_='article-listing')\n",
    "\n",
    "# Emty lists to hold the article data\n",
    "titles = []\n",
    "dates = []\n",
    "authors = []\n",
    "\n",
    "# Iterate through each article to extract the required information\n",
    "for article in articles:\n",
    "    # Extract the title\n",
    "    title_element = article.find('h2', class_='h5 article-title')\n",
    "    title = title_element.text.strip() if title_element else \"No title found\"\n",
    "    \n",
    "    # Extract the publication date\n",
    "    date_element = article.find('p', class_='article-date')\n",
    "    date = date_element.text.strip() if date_element else \"No date found\"\n",
    "    \n",
    "    # Extract the author(s)\n",
    "    author_element = article.find('p', class_='article-authors')\n",
    "    author = author_element.text.strip() if author_element else \"No author found\"\n",
    "    \n",
    "    # Append the extracted data to the respective lists\n",
    "    titles.append(title)\n",
    "    dates.append(date)\n",
    "    authors.append(author)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = {\n",
    "    'Title': titles,\n",
    "    'Date': dates,\n",
    "    'Author': authors\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# printing the data collected\n",
    "print( df )\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('most_downloaded_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d27708c-6429-45ba-854c-d37ba8f83e89",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
