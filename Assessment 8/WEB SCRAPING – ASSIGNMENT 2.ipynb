{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9452598-e65c-4cf6-ab7b-bdfcdf484759",
   "metadata": {},
   "source": [
    "<center><h1>WEB SCRAPING – ASSIGNMENT 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846ff1-d3fc-411d-a484-5e71982b8178",
   "metadata": {},
   "source": [
    "<p>Instructions <br>\n",
    "1. All the questions must be done in a single Jupyternotebook. <br>\n",
    "2. There should be proper comments in code<br>\n",
    "3. All the scraped date is stored in relavent CSV file. (For easy evaluation)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad733853-563c-48ad-aa58-c48ace905955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\noticed aaryan\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install webdriver_manager  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6726af-2d92-406b-a72e-db6b5fa17ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necacery library\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import InvalidSelectorException\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29890955-389d-485d-acfc-d45c97de67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open the browser and navigate to the given URL\n",
    "def open_browser(url):\n",
    "    try:\n",
    "        # Set up the webdriver\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening the browser: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df85b3-3394-4c5c-9db6-b5fca8813417",
   "metadata": {},
   "source": [
    "<p>Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and \n",
    "salary filter. <br>\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. <br>\n",
    "You have to scrape the job-title, job-location, company name, experience required. <br>\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs <br>\n",
    "The task will be done as shown in the below steps:  <br>\n",
    "1. first get the web page https://www.naukri.com/ <br>\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.  <br>\n",
    "3. Then click the search button.  <br>\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes  <br>\n",
    "5. Then scrape the data for the first 10 jobs results you get.  <br>\n",
    "6. Finally create a dataframe of the scraped data.  <br>\n",
    "<b>Note: All of the above steps have to be done in code. No step is to be done manually.</b> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3895541b-9630-4d61-a938-eb5de33e4e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Job Title  \\\n",
      "0    Data Scientist - Product Supply   \n",
      "1                     Data Scientist   \n",
      "2  Data Scientist/Data Analyst - LLM   \n",
      "3                       Data Analyst   \n",
      "4                       Data Analyst   \n",
      "5                                      \n",
      "6                                      \n",
      "7                                      \n",
      "8                     Data Scientist   \n",
      "9                     Data scientist   \n",
      "\n",
      "                                            Location  \\\n",
      "0  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "1                                              Noida   \n",
      "2  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "3                                          New Delhi   \n",
      "4                                           Gurugram   \n",
      "5                                Gurugram, Bengaluru   \n",
      "6                                              Noida   \n",
      "7                                              Noida   \n",
      "8                                           Gurugram   \n",
      "9  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "\n",
      "                       Company Experience  \n",
      "0                           PG    2-5 Yrs  \n",
      "1         Biz Tech Consultants    3-8 Yrs  \n",
      "2                 Hexaconcepts    2-6 Yrs  \n",
      "3                     Sociomix    0-5 Yrs  \n",
      "4                 Growthjockey    0-1 Yrs  \n",
      "5                         PayU    2-7 Yrs  \n",
      "6                   Innovaccer    2-7 Yrs  \n",
      "7               Times Internet    3-8 Yrs  \n",
      "8  Varuna Integrated Logistics    3-8 Yrs  \n",
      "9         Intense Technologies    3-7 Yrs  \n"
     ]
    }
   ],
   "source": [
    "# Opening the Naukri website\n",
    "driver = open_browser('https://www.naukri.com/')\n",
    "\n",
    "try:\n",
    "    # searching for 'Data Scientist'\n",
    "    designation = driver.find_element(By.CLASS_NAME, 'suggestor-input')\n",
    "    designation.send_keys('Data Scientist')\n",
    "\n",
    "    # clicking 'search button'\n",
    "    search = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "    search.click()\n",
    "\n",
    "    time.sleep(5) # Waiting\n",
    "\n",
    "    # Applying Location filters\n",
    "    location_catagory = driver.find_element(By.XPATH, \"(//i[@class='ni-icon-unchecked'])[10]\")\n",
    "    location_catagory.click()\n",
    "\n",
    "    time.sleep(5)  # Waiting\n",
    "\n",
    "    # Applying salary filter from 30k to 50k\n",
    "    salary_catogory = driver.find_element(By.XPATH, \"/html/body/div[1]/div/main/div[1]/div[1]/div/div/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "    salary_catogory.click()\n",
    "\n",
    "    time.sleep(5)  # Waiting\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while navigating:\", e )\n",
    "\n",
    "# List to store job details\n",
    "jobs = []\n",
    "\n",
    "# Extracting the job details\n",
    "try:\n",
    "    # Collect data for the first 10 jobs\n",
    "    for i in range(10):\n",
    "        # Extracting job details and storeing it in a list of dictionaries\n",
    "        title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title \"]')\n",
    "        location_tags = driver.find_elements(By.XPATH, '//span[@class=\"locWdth\"]')\n",
    "        company_tags = driver.find_elements(By.XPATH, '//div[@class=\" row2\"]/span/a[1]')\n",
    "        experience_tags = driver.find_elements(By.XPATH, '//span[@class=\"expwdth\"]')\n",
    "\n",
    "        jobs.append({\n",
    "            'Job Title': title_tags[i].text.strip(),\n",
    "            'Location': location_tags[i].text.strip(),\n",
    "            'Company': company_tags[i].text.strip(),\n",
    "            'Experience': experience_tags[i].text.strip() \n",
    "        })\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while extracting data:\", e)\n",
    "\n",
    "# Createing a DataFrame\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('naukri_jobs.csv', index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6806f-fbb4-4a9a-bff1-75bda359a3ee",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00b172-a14c-47c9-a04c-019343a9804c",
   "metadata": {},
   "source": [
    "<p>Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the <br> \n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.<br> \n",
    "This task will be done in following steps:<br> \n",
    "1. First get the webpage https://www.shine.com/<br> \n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.<br> \n",
    "3. Then click the searchbutton. <br> \n",
    "4. Then scrape the data for the first 10 jobs results you get.<br> \n",
    "5. Finally create a dataframe of the scraped data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bfc8705-7d38-4145-aa28-e0805ee7fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Job Title       Location  \\\n",
      "0  Data Scientist In SINGAPORE  Bangalore\\n+1   \n",
      "1               Data Scientist      Bangalore   \n",
      "2            Jr Data Scientist  Bangalore\\n+2   \n",
      "3               Data Scientist  Bangalore\\n+6   \n",
      "4               Data Scientist  Bangalore\\n+9   \n",
      "5               Data Scientist      Bangalore   \n",
      "6       Data Science Analytics  Bangalore\\n+7   \n",
      "7                 Data Analyst      Bangalore   \n",
      "8                 Data Analyst  Bangalore\\n+4   \n",
      "9      Snowflake Data Engineer  Bangalore\\n+2   \n",
      "\n",
      "                                  Company   Experience  \n",
      "0                   adal immigrations llp   3 to 8 Yrs  \n",
      "1  phoenix global re settlement servic...  6 to 11 Yrs  \n",
      "2  diraa hr services hiring for diraa ...   0 to 4 Yrs  \n",
      "3                           techno endura   0 to 2 Yrs  \n",
      "4                  future solution centre   2 to 7 Yrs  \n",
      "5  eliterecruitments hiring for global...   4 to 8 Yrs  \n",
      "6  mackenzie modern it solutions priva...   5 to 8 Yrs  \n",
      "7  phoenix global re settlement servic...   3 to 8 Yrs  \n",
      "8                        aryan technology   0 to 4 Yrs  \n",
      "9  capgemini technology services india...   4 to 9 Yrs  \n"
     ]
    }
   ],
   "source": [
    "# opening Shine website\n",
    "driver = open_browser('https://www.shine.com/')\n",
    "time.sleep(5)\n",
    "\n",
    "# Navigating to the search results page\n",
    "try:\n",
    "    # opening input prompt.\n",
    "    driver.find_element(By.CLASS_NAME,\"input\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Entering \"Data Scientist\"\n",
    "    driver.find_element(By.XPATH, \"//input[@id='id_q']\").send_keys('Data Scientist')\n",
    "    time.sleep(2) # Waiting\n",
    "\n",
    "    # Entering \"Bangalore\"\n",
    "    driver.find_element(By.ID, 'id_loc').send_keys('Bangalore')\n",
    "    time.sleep(2) # Waiting\n",
    "\n",
    "    # Clicking the search button\n",
    "    driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "    time.sleep(2) # Waiting\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while navigating: \", e)\n",
    "\n",
    " # Creating a empty list to hold the data\n",
    "jobs = []\n",
    "    \n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Scrape job data for the first 10 results\n",
    "        job_titles = driver.find_elements(By.XPATH, \"//strong[@class='jobCard_pReplaceH2__xWmHg']\")\n",
    "        locations = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2']\")\n",
    "        companies = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_cName__mYnow']\")\n",
    "        experiences = driver.find_elements(By.XPATH, \"//div[@class=' jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t']\")\n",
    "        \n",
    "        # Append the scraped data to the jobs list\n",
    "        job = {\n",
    "            'Job Title': job_titles[i].text,\n",
    "            'Location': locations[i].text,\n",
    "            'Company': companies[i].text,\n",
    "            'Experience': experiences[i].text\n",
    "        }\n",
    "        jobs.append(job)\n",
    "\n",
    "except Exception:\n",
    "    print(\"Error occurred while scraping job data.\")\n",
    "\n",
    "\n",
    "# Create a DataFrame and print the data\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Printing the results\n",
    "print(df)\n",
    "\n",
    "# Saving the results to a CSV file\n",
    "df.to_csv('shine_job.csv', index=False)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31825956-5ec7-498b-8036-d35c9e35211f",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34813b-a991-4333-8235-5ee8b1fb2d42",
   "metadata": {},
   "source": [
    "<p>Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=F\n",
    "LIPKART</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e7ff4-bc3f-48da-bea3-07c1556554b1",
   "metadata": {},
   "source": [
    "<img src= \"asset/flip.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13eb72-5d37-4a73-98ef-ef81ce7a91f4",
   "metadata": {},
   "source": [
    "<p>As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating<br>\n",
    "2. Review summary<br>\n",
    "3. Full review<br>\n",
    "4. You have to scrape this data for first 100reviews.<br>\n",
    "Note: All the stepsrequired during scraping should be done through code only and not manually.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79661bce-f5e4-4b82-8230-4bfe1f51b562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rating       Review Summary  \\\n",
      "0      5  Best in the market!   \n",
      "1      5             Terrific   \n",
      "2      5     Perfect product!   \n",
      "3      5       Classy product   \n",
      "4      5            Must buy!   \n",
      "\n",
      "                                         Full Review  \n",
      "0                                        Good Camera  \n",
      "1                                     Very very good  \n",
      "2                                       Photos super  \n",
      "3  Camera is awesome\\nBest battery backup\\nA perf...  \n",
      "4                                It’s really awesome  \n",
      "   Rating    Review Summary                                        Full Review\n",
      "95      5  Perfect product!                         Very nice iPhone 11 i lake\n",
      "96      5         Brilliant                                    Fantastic phone\n",
      "97      5         Must buy!                                          Excellent\n",
      "98      5          Terrific  Really worth of money. i just love it. It is t...\n",
      "99      5  Perfect product!  Iphone is just awesome.. battery backup is ver...\n"
     ]
    }
   ],
   "source": [
    "# Navigate to the iPhone 11 review page on Flipkart\n",
    "driver = open_browser(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "# The given link was not working.\n",
    "\n",
    "# Function to scrape reviews from the current page\n",
    "def scrape_reviews():\n",
    "    review_summaries   = driver.find_elements(By.XPATH, \"//p[@class='z9E0IG']\") \n",
    "    ratings            = driver.find_elements(By.XPATH, \"//div[@class='XQDdHH Ga3i8K']\")  \n",
    "    full_reviews       = driver.find_elements(By.XPATH, \"//div[@class='ZmyHeo']\")  \n",
    "\n",
    "    # A list to hold all for the screaped data in list for dict format.\n",
    "    reviews     = []\n",
    "    num_reviews = min(len(ratings),\n",
    "                      len(review_summaries),\n",
    "                      len(full_reviews))\n",
    "    for i in range(num_reviews):\n",
    "        reviews.append({\n",
    "            'Rating': ratings[i].text,\n",
    "            'Review Summary': review_summaries[i].text,\n",
    "            'Full Review': full_reviews[i].text\n",
    "        })\n",
    "    return reviews\n",
    "\n",
    "# collecting all the reviews into one place and limiting it to 100\n",
    "all_reviews = []\n",
    "# Continue until we have 10 reviews\n",
    "while len(all_reviews) < 100:  \n",
    "    # Scrape reviews from the current page\n",
    "    all_reviews.extend(scrape_reviews())\n",
    "    \n",
    "    # Click \"Next\" and proceed\n",
    "    if len(all_reviews) < 100:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"(//span[normalize-space()='Next'])[1]\")\n",
    "            next_button.click()\n",
    "            time.sleep(5)  # Waiting\n",
    "        except Exception as e:\n",
    "            print(\"error while continuing to Next page :\", e)\n",
    "            break \n",
    "\n",
    "# Create a DataFrame to store the reviews\n",
    "df_reviews = pd.DataFrame(all_reviews)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_reviews.head())\n",
    "print(df_reviews.tail())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_reviews.to_csv('df_reviews.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616fcd6c-3b24-4b7e-ad37-03ce57a890fc",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25627e-dba4-4f92-890b-cbccc5b7eb54",
   "metadata": {},
   "source": [
    "<p>Q4: Scrape data forfirst 100 sneakers you find when you visit flipkart.com and search for “sneakers” inthe search\n",
    "field.<br>\n",
    "You have to scrape 3 attributes of each sneaker:<br>\n",
    "1. Brand<br>\n",
    "2. ProductDescription<br>\n",
    "3. Price<br>\n",
    "As shown in the below image, you have to scrape the above attributes.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d1f97b-5c7a-4f28-bf0e-66a4cc22631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Brand                                Product Description   Price\n",
      "0      aadi  Synthetic Leather |Lightweight|Comfort|Summer|...    ₹299\n",
      "1  BERSACHE  Sneaker, Loafers ,Casual With Extra Comfort Sn...    ₹663\n",
      "2  BERSACHE  Bersache Sneaker, Loafers ,Casual With Extra C...    ₹759\n",
      "3      PUMA                        Puma Slash Sneakers For Men  ₹1,075\n",
      "4  BERSACHE  Sneaker, Loafers ,Casual With Extra Comfort Sn...    ₹704\n",
      "                    Brand                                Product Description  \\\n",
      "95               Skechers                   SKECH-AIR COURT Sneakers For Men   \n",
      "96               BUCKAROO                                   Sneakers For Men   \n",
      "97            bacca bucci  SUPER RARE Men's Retro Color Blocked Light Wei...   \n",
      "98  HRX by Hrithik Roshan                 Rerooted Classics Sneakers For Men   \n",
      "99               Skechers                ARCH FIT TRAIL AIR Sneakers For Men   \n",
      "\n",
      "     Price  \n",
      "95  ₹3,999  \n",
      "96  ₹2,209  \n",
      "97  ₹1,301  \n",
      "98  ₹1,047  \n",
      "99  ₹4,749  \n"
     ]
    }
   ],
   "source": [
    "# Opening the Flipkart website\n",
    "driver = open_browser('https://www.flipkart.com')\n",
    "\n",
    "# Waiting\n",
    "time.sleep(2)\n",
    "\n",
    "# Search for sneakers\n",
    "search_input = driver.find_element(By.NAME, 'q')\n",
    "search_input.send_keys('sneakers')\n",
    "search_input.send_keys('\\n')\n",
    "\n",
    "time.sleep(3)  # Waiting\n",
    "\n",
    "# Creating empty lists\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Initial page number\n",
    "i = 1\n",
    "\n",
    "# Loop to scrape data until 100 sneakers are gathered\n",
    "while len(brands) < 100:\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find sneaker blocks\n",
    "    sneaker_blocks = soup.find_all('div', class_='hCKiGj')\n",
    "\n",
    "    for block in sneaker_blocks:\n",
    "        if len(brands) >= 100:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Extract the brand, description, and price using BeautifulSoup\n",
    "            brand = block.find('div', class_='syl9yP').text.strip()  # Finding the brand\n",
    "            description = block.find('a', class_='WKTcLC BwBZTg').text.strip()  # Finding the product description\n",
    "            price = block.find('div', class_='Nx9bqj').text.strip()  # Finding the price\n",
    "\n",
    "            # Append data to lists\n",
    "            brands.append(brand)\n",
    "            descriptions.append(description)\n",
    "            prices.append(price)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Navigate to the next page if needed\n",
    "    try:\n",
    "        i += 1\n",
    "        next_button = driver.find_element(By.XPATH, f\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[{i}]\")  # Assuming this points to the next page\n",
    "        if next_button:\n",
    "            next_button.click()\n",
    "            time.sleep(3)  # Waiting\n",
    "    except Exception as e:\n",
    "        print(f\"Next page navigation error: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Brand': brands,\n",
    "    'Product Description': descriptions,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('flipkart_sneakers.csv', index=False)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac325af-7fc1-4b6a-ac43-39e3b003e232",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cb995-a115-4130-9be9-40cc67fcd95d",
   "metadata": {},
   "source": [
    "<p>Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU<br>\n",
    "Type filter to “Intel Core i7” as shown in the below image:<br>\n",
    "Aftersetting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:<br>\n",
    "1. Title<br>\n",
    "2. Ratings<br>\n",
    "3. Price</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5594826f-9e4d-44c2-aef2-f7d9c776d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title              Rating  \\\n",
      "0  ASUS Vivobook 15, 15.6\" (39.62cm) FHD, Intel C...  3.5 out of 5 stars   \n",
      "1  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...  4.0 out of 5 stars   \n",
      "2  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...  3.7 out of 5 stars   \n",
      "3  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...  4.1 out of 5 stars   \n",
      "4  Acer Travelmate Business Laptop Intel Core i7-...  3.1 out of 5 stars   \n",
      "5  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...          No ratings   \n",
      "6  MSI Thin 15, Intel 12th Gen. Core i7-12650H, 4...  4.1 out of 5 stars   \n",
      "7  ASUS TUF Gaming F15, 15.6\" (39.62cm) FHD 144Hz...  4.0 out of 5 stars   \n",
      "8  Dell Inspiron 7440 Thin & Light Laptop, Intel ...          No ratings   \n",
      "9  MSI Thin 15, Intel 12th Gen. Core i7-12650H, 4...  4.5 out of 5 stars   \n",
      "\n",
      "    Price  \n",
      "0  56,990  \n",
      "1  64,990  \n",
      "2  47,990  \n",
      "3  76,990  \n",
      "4  39,990  \n",
      "5  49,990  \n",
      "6  64,995  \n",
      "7  99,990  \n",
      "8  81,990  \n",
      "9  61,990  \n"
     ]
    }
   ],
   "source": [
    "# Opening Amazon\n",
    "driver = open_browser('https://www.amazon.in/')\n",
    "\n",
    "# searching for \"Laptop\"\n",
    "search_box = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "search_box.send_keys('Laptop')\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Waiting\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking the 'Intel Core i7' filter\n",
    "intel_i7_filter = driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\")\n",
    "intel_i7_filter.click()\n",
    "\n",
    "# Waiting\n",
    "time.sleep(3)\n",
    "\n",
    "# Scrape the first 10 laptop's detail's\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Finding all the laptop divs that contain the required data\n",
    "laptops = soup.find_all('div', {'data-component-type': 's-search-result'})[:10]  # Limit to 10 results\n",
    "\n",
    "# Creating a empty list\n",
    "laptop_data = []\n",
    "\n",
    "# Extract title, ratings, and price for each laptop\n",
    "for laptop in laptops:\n",
    "    title = laptop.h2.text.strip()  # Extracting title\n",
    "    rating = laptop.find('span', {'class': 'a-icon-alt'})  # Extrating ratings\n",
    "    rating = rating.text.strip() if rating else 'No ratings' # Filtering\n",
    "    price = laptop.find('span', {'class': 'a-price-whole'})  # Extracting price\n",
    "    price = price.text.strip() if price else 'Price not available' # Filtering\n",
    "\n",
    "    # Append the data to the list of dictionaries\n",
    "    laptop_data.append({\n",
    "        'Title': title,\n",
    "        'Rating': rating,\n",
    "        'Price': price\n",
    "    })\n",
    "\n",
    "# Creating a DataFrame \n",
    "df = pd.DataFrame(laptop_data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('list_of_Intel_Core_i7_laptops.csv', index=False)\n",
    "print(df)\n",
    "\n",
    "# Closing the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7985e6-574d-4454-9348-430f84574ab1",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68c3bc-1dff-4d66-84fd-057e0f8d6b09",
   "metadata": {},
   "source": [
    "<p>Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.<br>\n",
    "The above task will be done in following steps:<br>\n",
    "1. First get the webpagehttps://www.azquotes.com/<br>\n",
    "2. Click on TopQuote<br>\n",
    "3. Than scrap a)Quote b) Author c) Type Of Quotes<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d688120-ca55-467a-9f89-9bbc60893b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Quote              Author  \\\n",
      "0  The essence of strategy is choosing what not t...      Michael Porter   \n",
      "1  One cannot and must not try to erase the past ...          Golda Meir   \n",
      "2  Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
      "3  Death is something inevitable. When a man has ...      Nelson Mandela   \n",
      "4  You have to love a nation that celebrates its ...        Erma Bombeck   \n",
      "\n",
      "                                 Type/Genre  \n",
      "0  Essence, Deep Thought, Transcendentalism  \n",
      "1                 Inspiration, Past, Trying  \n",
      "2                       Country, Peace, War  \n",
      "3        Inspirational, Motivational, Death  \n",
      "4              4th Of July, Food, Patriotic  \n",
      "                                                 Quote              Author  \\\n",
      "995  Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
      "996  America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
      "997  For every disciplined effort there is a multip...            Jim Rohn   \n",
      "998  The spiritual journey is individual, highly pe...            Ram Dass   \n",
      "999  The mind is not a vessel to be filled but a fi...            Plutarch   \n",
      "\n",
      "                                Type/Genre  \n",
      "995      Love, Inspirational, Motivational  \n",
      "996                 Gun, Two, Qualms About  \n",
      "997  Inspirational, Greatness, Best Effort  \n",
      "998                 Spiritual, Truth, Yoga  \n",
      "999   Inspirational, Leadership, Education  \n"
     ]
    }
   ],
   "source": [
    "# Opening the website\n",
    "url = \"https://www.azquotes.com/\"\n",
    "driver = open_browser(url)\n",
    "\n",
    "# Clicking on \"Top Quotes\" button \n",
    "top_quotes_button = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "top_quotes_button.click()\n",
    "\n",
    "# Waiting\n",
    "time.sleep(3)\n",
    "\n",
    "# creating a empty list\n",
    "quotes_list = []\n",
    "\n",
    "# Scraping 1000 quotes ;)\n",
    "while len(quotes_list) < 1000:\n",
    "\n",
    "    time.sleep(2)  # waiting\n",
    "\n",
    "    # using beautifulscoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find the quotes\n",
    "    quotes = soup.find_all('div', class_='wrap-block')\n",
    "    \n",
    "    for quote in quotes:\n",
    "        if len(quotes_list) >= 1000:  # Stop if we already have 1000 quotes\n",
    "            break\n",
    "        try:\n",
    "            # Scraping the quote text\n",
    "            quote_text = quote.find('a', class_='title').text.strip()\n",
    "\n",
    "            # Scraping the author\n",
    "            author = quote.find('div', class_='author').text.strip()\n",
    "            \n",
    "            # Scraping the type of quote\n",
    "            type_of_quote = quote.find('div', class_='tags').text.strip()\n",
    "\n",
    "            # Storing the data in a list of dictionary\n",
    "            quotes_list.append({\n",
    "                'Quote': quote_text,\n",
    "                'Author': author,\n",
    "                'Type/Genre': type_of_quote\n",
    "            })\n",
    "\n",
    "        # Skip if any required data is not found   \n",
    "        except AttributeError:\n",
    "            continue \n",
    "\n",
    "    # Try to click the \"Next\" button for pagination\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"(//a[contains(text(),'Next →')])[1]\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "\n",
    "    # Break out of the loop if error occurs   \n",
    "    except Exception:\n",
    "        break \n",
    "\n",
    "# Convert the scraped data into a DataFrame\n",
    "df = pd.DataFrame(quotes_list)\n",
    "\n",
    "# Showing the DataFrame with no index :^] \n",
    "print( df.head(5) )\n",
    "print( df.tail(5) )\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('azquotes_top_1000.csv', index=False)\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733db01a-b0ed-410a-b76d-2d45cb757c12",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7aed7-bbce-476a-b0ac-8b6ce1c02d7b",
   "metadata": {},
   "source": [
    "Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name,\n",
    "Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of\u0002all-prime-ministers-of-india-1473165149-1\n",
    "scrap the mentioned data and make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568ae588-aa1b-441b-a4f3-4a6566a47830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name Age when assumed office Term of Office (from)  \\\n",
      "0          Jawaharlal Nehru      57 years, 274 days         15 August1947   \n",
      "1          Gulzarilal Nanda      65 years, 328 days           27 May 1964   \n",
      "2       Lal Bahadur Shastri      62 years, 250 days           9 June 1964   \n",
      "3          Gulzarilal Nanda      67 years, 191 days       11 January 1966   \n",
      "4             Indira Gandhi       48 years, 66 days       24 January 1966   \n",
      "5             Morarji Desai       81 years, 24 days         24 March 1977   \n",
      "6              Charan Singh      76 years, 217 days          28 July 1979   \n",
      "7             Indira Gandhi       62 years, 56 days       14 January 1980   \n",
      "8              Rajiv Gandhi       40 years, 72 days       31 October 1984   \n",
      "9   Vishwanath Pratap Singh      58 years, 160 days       2 December 1989   \n",
      "10          Chandra Shekhar      63 years, 207 days      10 November 1990   \n",
      "11      P. V. Narasimha Rao      69 years, 358 days          21 June 1991   \n",
      "12     Atal Bihari Vajpayee      71 years, 143 days           16 May 1996   \n",
      "13         H. D. Deve Gowda       63 years, 14 days           1 June 1996   \n",
      "14       Inder Kumar Gujral      77 years, 138 days         21 April 1997   \n",
      "15     Atal Bihari Vajpayee       73 years, 84 days         19 March 1998   \n",
      "16           Manmohan Singh      71 years, 239 days           22 May 2004   \n",
      "17            Narendra Modi      63 years, 251 days           26 May 2014   \n",
      "\n",
      "   Term of Office (to)             Remarks  \n",
      "0          27 May 1964  16 years, 286 days  \n",
      "1          9 June 1964             13 days  \n",
      "2      11 January 1966    1 year, 216 days  \n",
      "3      24 January 1966             13 days  \n",
      "4        24 March 1977   11 years, 59 days  \n",
      "5         28 July 1979   2 years, 126 days  \n",
      "6      14 January 1980            170 days  \n",
      "7      31 October 1984   4 years, 291 days  \n",
      "8      2 December 1989    5 years, 32 days  \n",
      "9     10 November 1990            343 days  \n",
      "10        21 June 1991            223 days  \n",
      "11         16 May 1996   4 years, 330 days  \n",
      "12         1 June 1996             16 days  \n",
      "13       21 April 1997            324 days  \n",
      "14       19 March 1998            332 days  \n",
      "15         22 May 2004    6 years, 64 days  \n",
      "16         26 May 2014    10 years, 4 days  \n",
      "17           Incumbent   10 years, 87 days  \n"
     ]
    }
   ],
   "source": [
    "# Opening the Jagran Josh page for Prime Ministers of India\n",
    "driver = open_browser('https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1')\n",
    "\n",
    "time.sleep(3)  # Waiting\n",
    "\n",
    "# Scrape the table data\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "pm_list = []\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')[1:]  # Skiped the header row\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all('td')\n",
    "    name = columns[1].text.strip()\n",
    "    born_dead = columns[2].text.strip()\n",
    "    term_from = columns[3].text.strip()\n",
    "    term_to = columns[4].text.strip()\n",
    "    remarks = columns[5].text.strip()\n",
    "\n",
    "    pm_list.append([ name ,\n",
    "                     born_dead ,\n",
    "                     term_from ,\n",
    "                     term_to ,\n",
    "                     remarks\n",
    "                    ])\n",
    "\n",
    "# Create a DataFrame \n",
    "columns = ['Name',\n",
    "           'Age when assumed office',\n",
    "           'Term of Office (from)',\n",
    "           'Term of Office (to)',\n",
    "           'Remarks']\n",
    "df = pd.DataFrame(pm_list, columns=columns )\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('prime_ministers_of_india.csv', index=False)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403d1fc-af92-47eb-b685-7e293b3f25d4",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26756b-6c5d-472f-9925-d89cfc4c1754",
   "metadata": {},
   "source": [
    "Q8: Write a python program to display list of 50 Most expensive cars in the world\n",
    "(i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap thementioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b73434d-490f-474d-a43b-14e7a0d64423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Car Name                Price\n",
      "0             Hennessey Venom F5  Price: $1.8 Million\n",
      "1                Bentley Bacalar  Price: $1.9 Million\n",
      "2  Hispano Suiza Carmen Boulogne  Price: $1.9 Million\n",
      "3         Bentley Mulliner Batur  Price: $2.0 Million\n",
      "4                    SSC Tuatara  Price: $2.0 Million\n",
      "                              Car Name                        Price\n",
      "45             Bugatti Chiron Profilée         Price: $10.8 Million\n",
      "46                Rolls-Royce Sweptail         Price: $12.8 Million\n",
      "47            Bugatti La Voiture Noire         Price: $13.4 Million\n",
      "48               Rolls-Royce Boat Tail  Price: $28.0 Million (est.)\n",
      "49  Rolls-Royce La Rose Noire Droptail    Price: $30 Million (est.)\n"
     ]
    }
   ],
   "source": [
    "# Passing URL into open_browser function\n",
    "url = 'https://www.motor1.com/'\n",
    "driver = open_browser(url)\n",
    "\n",
    "# Set up the webdriver\n",
    "wait = WebDriverWait(driver, 5) \n",
    "try:\n",
    "    # Searching for \"50 most expensive cars\"\n",
    "    search_field = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"search_input\"]'))\n",
    "    )\n",
    "    search_field.send_keys(\"50 most expensive cars\")\n",
    "    search_field.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Waiting\n",
    "\n",
    "    # Clicking on the link that leads to the list of most expensive cars\n",
    "    car_list_link = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/div[6]/div[9]/div/div[1]/div/div/div[1]/div\"))  \n",
    "    )\n",
    "    car_list_link.click()\n",
    "    time.sleep(3)  # Waiting\n",
    "except Exception as e:\n",
    "    print(f\"Error during search and navigation: {e}\")\n",
    "\n",
    "# Create empty lists for each feature\n",
    "cars = []\n",
    "prices = []\n",
    "\n",
    "try:\n",
    "    time.sleep(5)# Waiting\n",
    "    \n",
    "    # Searching for names and prices of all expencive cars\n",
    "    car_elements = driver.find_elements(By.CLASS_NAME, \"subheader\") \n",
    "    price_elements = driver.find_elements(By.XPATH, '//p/strong[contains(text(), \"$\")]')  \n",
    "\n",
    "    # Ensure we get all the results and append them to the empty list.\n",
    "    for car, price in zip(car_elements, price_elements):\n",
    "        cars.append(car.text.strip())\n",
    "        prices.append(price.text.strip())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during scraping: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# saving the results into a CSV file.\n",
    "if cars and prices:\n",
    "    df = pd.DataFrame({\n",
    "        'Car Name': cars,\n",
    "        'Price': prices\n",
    "    })\n",
    "    df.to_csv('most_expensive_cars.csv', index=False)\n",
    "    print(df.head(5))\n",
    "    print(df.tail(5))\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d38f1a-70bd-4460-9a0e-54ffe7f81982",
   "metadata": {},
   "source": [
    "*************************************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
